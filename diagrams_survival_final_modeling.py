# -*- coding: utf-8 -*-
"""Diagrams Survival Final Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LgmVIQfqNJGZQE3v_jHdkrEy_9Z7QZAY
"""

! pip install scipy
! pip install scikit-survival
! pip install lifelines

# ! pip install combat
# ! pip install SimpleITK
# ! pip install scikit-survival
# ! pip install lifelines
'''
from combat.pycombat import pycombat
import SimpleITK as sitk
from lifelines.utils import concordance_index
from lifelines.statistics import logrank_test
from lifelines import CoxPHFitter
from lifelines.utils import datetimes_to_durations
from sksurv.ensemble import RandomSurvivalForest
from lifelines import KaplanMeierFitter
from lifelines.datasets import load_waltons
from lifelines.utils import k_fold_cross_validation
'''

import os
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LassoCV, Lasso
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import mean_absolute_error
from math import sqrt
from sklearn.model_selection import RandomizedSearchCV as RSCV
from sklearn.model_selection import GridSearchCV as GSCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from scipy.stats import kurtosis
from sklearn.preprocessing import LabelEncoder
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
import statsmodels.api as sm
from sksurv.ensemble import RandomSurvivalForest
from sksurv.metrics import concordance_index_censored
from sksurv.util import Surv
from scipy import stats
from statsmodels.multivariate import tests
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from scipy.stats import levene
from sklearn.naive_bayes import GaussianNB
from lifelines import KaplanMeierFitter

from google.colab import drive
# access to google drive
drive.mount('/content/drive', force_remount=True)

# Use cell to combine final radiomic data to final clinic data
# also used to combine numerous other datasets
df1 = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalPCA.csv')
df2 = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/IDClinRiskFactorOnManualStratify.csv')
merged_df = pd.merge(df1, df2, on='Patient-ID')
merged_df.to_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalHybridDATASET.csv', index=False)

# stratified risk scores made utilizing Fong Score and Kawaguchi et al, will make comparable hazard models
# 0 is considered low risk in first 2 year period relatively to 1, which is high risk in first two years post operation
'''
def assign_risk_levels(input_csv_path, output_csv_path):

    df = pd.read_csv(input_csv_path)


    risk_factor_columns = ['node_positive_primary', 'max_diameter_5cm',
                           'Multiple_CLM', 'DFS_12',
                           'CEA_200']

    for col in risk_factor_columns:
        if col not in df.columns:
            raise ValueError(f"'{col}' column not found in the DataFrame.")


    df['Recurrence_Risk'] = df[risk_factor_columns].sum(axis=1)


    conditions = [
        (df['Recurrence_Risk'] >= 0) & (df['Recurrence_Risk'] <= 2),
        (df['Recurrence_Risk'] > 2) & (df['Recurrence_Risk'] <= 5)
    ]
    choices = [0,1]

    df['stratified_risk'] = pd.Series(np.select(conditions, choices, default='unknown'))


    df.to_csv(output_csv_path, index=False)
    print(f"Updated data with risk classification saved to {output_csv_path}")


input_csv_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/CleanedClinicAllInfo.csv'
output_csv_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/CleanedClinicAllInfo.csv'

assign_risk_levels(input_csv_path, output_csv_path)
'''

'''
def load_data(csv_file_path):

    df = pd.read_csv(csv_file_path)
    return df

def plot_correlation_matrix(df):

    plt.figure(figsize=(12, 8))


    correlation_matrix = df.corr()  # Calculate correlation


    sns.heatmap(correlation_matrix, annot=False, fmt=".2f", cmap='coolwarm', square=True, cbar=True)

    plt.title('Correlation Matrix')
    plt.show()

def main_correlation_analysis(csv_file_path):
    """Main function to load data and plot correlation matrix."""
    df = load_data(csv_file_path)


    df_numeric = df.select_dtypes(include=[np.number])


    plot_correlation_matrix(df_numeric)


csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/CorrMatrix.csv'

main_correlation_analysis(csv_file_path)
'''

'''
def binarize_risk_scores(risk_scores, cutoff=None):

    if cutoff is None:

        cutoff = risk_scores.median()


    binary_labels = (risk_scores > cutoff).astype(int)
    return binary_labels, cutoff

def main_risk_binarization(csv_file_path):


    df = pd.read_csv(csv_file_path)

    #risk scores are in the column 'Risk_Score'
    if 'Risk_Score' not in df.columns:
        raise ValueError("'Risk_Score' column not found in the DataFrame.")

    risk_scores = df['Risk_Score']


    binary_grouping, chosen_cutoff = binarize_risk_scores(risk_scores)


    df['Risk_Group'] = binary_grouping


    print(f"Chosen cutoff for binary grouping: {chosen_cutoff}")
    print("Patient IDs with their corresponding Risk Group:")
    print(df[['PatientID', 'Risk_Score', 'Risk_Group']])


    output_csv_path = '/path/to/your/binarized_output.csv'
    df.to_csv(output_csv_path, index=False)
    print(f"Binarized data saved to {output_csv_path}")


csv_file_path = '/path/to/your/risk_scores.csv'  # Update with your actual risk scores CSV path

main_risk_binarization(csv_file_path)
'''

# RSF with Risk scores made
'''
def load_data(csv_file_path):
    """Load and return the clinical data from a CSV file."""
    df = pd.read_csv(csv_file_path)
    return df

def preprocess_data(df):

    # 'vital_status' is 1 for deceased and 0 for alive
    # 'months_to_recurrence' defined for duration
    df['recurrence'] = df['progression_or_recurrence']
    df['duration'] = df['months_to_DFS_progression']


    return Surv.from_dataframe('recurrence', 'duration', df)

def train_random_forest_model(X, y):

    model = RandomSurvivalForest(n_estimators= 80, min_samples_split=2, min_samples_leaf=1, max_features=0.9, max_depth=9, bootstrap=False)
    model.fit(X, y)
    return model

def calculate_risk_scores(model, X):

    risk_scores = model.predict(X)  # Obtain risk scores or survival function
    return risk_scores

def main_random_forest_survival(csv_file_path):


    df = load_data(csv_file_path)


    y = preprocess_data(df)  #structured survival data
    X = df.drop(columns=['Patient-ID', 'progression_or_recurrence', 'months_to_DFS_progression','overall_survival_months', 'NASH_greater_4',
                         'fibrosis_greater_40_percent', 'max_diameter_5cm', 'DFS_12', 'CEA_200', 'stratified_risk', 'recurrence', 'duration'], errors='ignore')  # Features

    # Train Random Forest Survival Model
    model = train_random_forest_model(X, y)

    # Calculate Risk Scores
    risk_scores = calculate_risk_scores(model, X)


    df['Risk_Score'] = risk_scores
    print(df[['Patient-ID', 'Risk_Score']])


    output_csv_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/RSFRiskStratifyInfo.csv'
    df.to_csv(output_csv_path, index=False)
    print(f"Risk scores saved to: {output_csv_path}")
    model.fit(X,y)
    return model

def determine_cutoff(model, X):
    """Determine the cutoff for risk scores using ROC analysis."""
    risk_scores = model.predict(X)  # Get predicted risk scores


    y_true = np.array([1 if x['recurrence'] == 1 else 0 for x in y])  # Binary target based on survival status

    # Calculate ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, risk_scores, pos_label=1)

    # Youden's index to find optimal threshold
    youden_index = tpr - fpr
    best_index = np.argmax(youden_index)
    best_threshold = thresholds[best_index]

    #AUC
    roc_auc = auc(fpr, tpr)

    print(f"Optimal cutoff found at: {best_threshold}")
    print(f"AUC: {roc_auc:.2f}")

    return best_threshold

def main_analysis(csv_file_path):

    X, y = load_data(csv_file_path)


    model = main_random_forest_survival(X, y)


    optimal_cutoff = determine_cutoff(model, X)


csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/RSFRiskStratifyInfo.csv'

main_analysis(csv_file_path)


csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/RSFInputFinal.csv'

main_random_forest_survival(csv_file_path)
'''

'''
def load_data(csv_file_path):

    df = pd.read_csv(csv_file_path)
    return df

def calculate_survival_rate(df):

    df['overall_survival_rate'] = (df['overall_survival_months'] / df['months_to_DFS_progression']).fillna(0) * 100
    return df

def create_patient_subsets(df):

    cohort_48_df = df[df['months_to_DFS_progression'] >= 48].copy()
    cohort_24_df = df[df['months_to_DFS_progression'] >= 24].copy()
    entire_cohort_df = df.copy()


    cohort_48_df = calculate_survival_rate(cohort_48_df)
    cohort_24_df = calculate_survival_rate(cohort_24_df)
    entire_cohort_df = calculate_survival_rate(entire_cohort_df)

    return cohort_48_df, cohort_24_df, entire_cohort_df

def prepare_plot_data(df, label):
    """Data prep for plotting overall survival rate over a 120-month period."""
    plot_data = {'Months': [], 'Overall Survival Rate (%)': []}

    # Calculating averages for every 12-month step
    for month in range(0, 121, 12):  # From 0 to 120 months in steps of 12
        if len(df) > 0:
            count = len(df[df['months_to_DFS_progression'] >= month])
            plot_data['Months'].append(month)
            plot_data['Overall Survival Rate (%)'].append(count * 100 / len(df))  # Convert to percentage
        else:
            plot_data['Months'].append(month)
            plot_data['Overall Survival Rate (%)'].append(0)

    return pd.DataFrame(plot_data)

def plot_survival_graph(plot_data, label):

    plt.plot(plot_data['Months'], plot_data['Overall Survival Rate (%)'], marker='o', label=f'Overall Survival Rate - {label}')

    plt.title(f'Overall Survival Rate for {label} Subset over 120 Months')
    plt.xlabel('Months to DFS Progression')
    plt.ylabel('Overall Survival Rate (%)')
    plt.xticks(plot_data['Months'])
    plt.ylim(0, 100)
    plt.xlim(0, 120)
    plt.grid()
    plt.legend()
    plt.show()
    plt.savefig('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/Recurrence&SurvivalTrends.jpg')

csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/CleanedClinicAllInfo.csv'
df = load_data(csv_file_path)

# patient subsets
cohort_48_df, cohort_24_df, entire_cohort_df = create_patient_subsets(df)


plot_data_48 = prepare_plot_data(cohort_48_df, '4-years Recurrence Free')
plot_data_24 = prepare_plot_data(cohort_24_df, '2-Years Recurrence Free')
plot_data_entire = prepare_plot_data(entire_cohort_df, 'Entire Cohort')


plot_survival_graph(plot_data_48, '4-years Recurrence Free')
plot_survival_graph(plot_data_24, '2-Years Recurrence Free')
plot_survival_graph(plot_data_entire, 'Entire Cohort')
'''

"""Predictive Models

<><><><><><><><>
"""

'''
def load_data(csv_file_path):

    df = pd.read_csv(csv_file_path)
    return df

def preprocess_data(df):

    df['progression_or_recurrence'] = df['progression_or_recurrence'].astype('category').cat.codes  # Convert to categorical codes
    return df

def fit_multiclass_logistic_regression(df, target_column):

    X = df.drop(columns=['progression_or_recurrence'])
    y = df['progression_or_recurrence']


    X = sm.add_constant(X)


    model = sm.MNLogit(y, X)
    result = model.fit()

    return result

def extract_statistics(results):

    # Get a list of class labels (excluding the reference class)
    class_labels = results.model.J - 1  # J is the total number of classes

    summary_list = []  # To store results for each class

    for i in range(class_labels):
        # Calculate odds ratios for this class
        odds_ratios = np.exp(results.params.iloc[i + 1])  # Skip the intercept row

        # Confidence intervals for odds ratios
        conf = results.conf_int()
        conf = np.exp(conf)  # Transform CI to odds ratios
        ci_lower = conf.iloc[i + 1, 0]  # Lower CI for this class
        ci_upper = conf.iloc[i + 1, 1]  # Upper CI for this class


        p_values = results.pvalues.iloc[i + 1]

        class_summary = pd.DataFrame({
            'Feature': odds_ratios.index,
            'Odds Ratio': odds_ratios.values,
            'CI Lower': [ci_lower] * len(odds_ratios),
            'CI Upper': [ci_upper] * len(odds_ratios),
            'p-value': p_values.values,
            'Class': [i + 1] * len(odds_ratios)  # Add class label
        })
        summary_list.append(class_summary)


    summary = pd.concat(summary_list, ignore_index=True)

    return summary

def main_logistic_regression_analysis(csv_file_path):
    """Main function to perform analysis and retrieve significant features."""
    df = load_data(csv_file_path)

    df = preprocess_data(df)  # Preprocess as needed for your dataset


    target_column = 'progression_or_recurrence'
    results = fit_multiclass_logistic_regression(df, target_column)


    stats_summary = extract_statistics(results)


    significant_features = stats_summary[stats_summary['p-value'] < 0.05]

    print("Significant Features (p-value < 0.05):")
    print(significant_features)


csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/IDClinRiskFactorOnManualStratify.csv'  # Update with your CSV file path
main_logistic_regression_analysis(csv_file_path)
'''

'''
def load_data(csv_file_path):
    df = pd.read_csv(csv_file_path)

    df['status'] = df['progression_or_recurrence']  # Adjust according to your column names
    df['duration'] = df['months_to_DFS_progression']  # Your duration column


    y = Surv.from_dataframe('status', 'duration', df)


    X = df.drop(columns=['recurrence', 'months_to_recurrence', 'status', 'duration', 'Patient-ID'], errors='ignore')
    return X, y

def fit_random_survival_forest(X, y):
    model = RandomSurvivalForest(n_estimators= 80, min_samples_split=2, min_samples_leaf=1, max_features=0.9, max_depth=9, bootstrap=False)
    model.fit(X, y)
    return model

def plot_survival_and_hazard(model, X, y):
    """survival function and cumulative hazard function for all patients with unique (no intersection) colors."""

    survival_functions = model.predict_survival_function(X)


    durations = y['duration']
    events = y['status']


    kmf = KaplanMeierFitter()
    kmf.fit(durations, events)


    plt.figure(figsize=(12, 6))

    # Kaplan-Meier S-Function
    plt.subplot(1, 2, 1)
    plt.step(kmf.survival_function_.index, kmf.survival_function_,
             where="post", label='Kaplan-Meier Estimate', color='blue')
    plt.title('Recurrence Risk Function (Probability of Recurrence)')
    plt.xlabel('Months to Recurrence')
    plt.ylabel('Recurrence Probability')
    plt.xlim(0, durations.max())
    plt.ylim(0, 1)
    plt.grid()

    # individual survival functions plotting
    plt.subplot(1, 2, 2)
    num_patients = survival_functions.shape[0]
    colors = plt.cm.get_cmap('viridis', num_patients)  # Get a colormap

    for i, survival_function in enumerate(survival_functions):
        time_points = survival_function.x
        survival_probs = survival_function.y
        plt.step(time_points, survival_probs, where="post",
                 label=f'Patient {i+1}', color=colors(i), alpha=0.7)

    plt.title('Individual Survival Functions')
    plt.xlabel('Months to Recurrence')
    plt.ylabel('Recurrence Probability')
    plt.xlim(0, durations.max())
    plt.ylim(0, 1)
    plt.grid()

    plt.tight_layout()

    plt.show()
    plt.savefig('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/SurvivalFuncAllCRLM.png')

    plt.subplot(1, 2, 2)

    # cumulative hazard plot for each patient
    cumulative_hazard_functions = model.predict_cumulative_hazard_function(X)
    for chf in cumulative_hazard_functions:
        plt.step(chf.x, chf.y, where="post", color='gray', alpha=0.3)


    avg_chf_x = cumulative_hazard_functions[0].x  # Assume all have the same time points
    avg_chf_y = np.mean([chf.y for chf in cumulative_hazard_functions], axis=0)
    plt.step(avg_chf_x, avg_chf_y, where="post",
             label='Average Cumulative Hazard', color='orange', linewidth=2)

    plt.title('Cumulative Hazard Function (All Patients)')
    plt.xlabel('Months to Recurrence')
    plt.ylabel('Cumulative Hazard')
    plt.xlim(0, durations.max())
    # plt.ylim(0, kmf.cumulative_hazard_.max())  # Adjust ylim if needed
    plt.grid()

    plt.tight_layout()
    plt.show()
    plt.savefig('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/CumHazardAllCRLM.png')

csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/IDClinRiskFactorOnManualStratify.csv'
X, y = load_data(csv_file_path)

# Fit the Random Survival Forest model
model = fit_random_survival_forest(X, y)

# Plot survival and hazard functions
plot_survival_and_hazard(model, X, y)
'''

# hyperparameterization of RSF
'''
ins = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/RSFRiskStratifyInfo.csv')
X = ins.drop(['Patient-ID', 'progression_or_recurrence', 'recurrence', 'duration'], axis=1)
Y = ins['Risk_Score']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

rf_randomsearch = RandomForestRegressor(random_state=42)

# grid the different tuning methods for random forest
param_tuning = {
    'n_estimators':np.arange(20,100,10), #start,stop,step intervals
    'max_features':np.arange(0.3, 1, 0.1), # ^same:number of features per subset
    'max_depth': [3,5,7,9], # of tree levels
    'min_samples_split': [2,5,9], # min samples to split internal node
    'min_samples_leaf': [1,2,4],
    'bootstrap': [True, False] # min samples at leaf node
}

rand_search = RSCV(estimator=rf_randomsearch, param_distributions=param_tuning, n_iter=100, cv=5, n_jobs=-1, verbose=2, scoring='r2')
rand_search.fit(X_train, Y_train)

best_rf = rand_search.best_estimator_
y_pred = best_rf.predict(X_test)

mse = mean_squared_error(Y_test, y_pred)
mae = mean_absolute_error(Y_test, y_pred)
r2 = r2_score(Y_test, y_pred)

print(f"Best Hyperparameters: {rand_search.best_params_}")
print(f"Mean Squared Error: {mse}")
print(f"Mean Absolute Error: {mae}")
print(f"R2 Score: {r2}")
'''

'''
RSF = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/RSFRiskStratifyInfoHYP.csv')
X = RSF.drop(['Risk_Score', 'Patient-ID', 'progression_or_recurrence', 'duration', 'months_to_DFS_progression', 'recurrence'], axis=1)
Y = RSF['Risk_Score']

model = RandomForestRegressor(n_estimators= 80, min_samples_split=2, min_samples_leaf=1, max_features=0.9, max_depth=9, bootstrap=False)

model.fit(X, Y)
importances = model.feature_importances_
std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
indices = np.argsort(importances)[::-1]

print("Feature ranking:")
feature_names = X.columns

for f in range(X.shape[1]):
    print("%d. Feature %s (%f)" % (f + 1, feature_names[indices[f]], importances[indices[f]]))
'''

"""Models"""

'''
def load_data(csv_file_path):

    df = pd.read_csv(csv_file_path)


    if 'stratified_risk' not in df.columns:
        raise ValueError("target column not found in the DataFrame.")


    X = df.drop(columns=['stratified_risk'])  # All columns except 'target'
    y = df['stratified_risk']  # The target column

    return X, y

def calculate_performance_metrics(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)

    TN, FP, FN, TP = cm.ravel()
    sensitivity = TP / (TP + FN)

    specificity = TN / (TN + FP)


    positive_predictive_value = TP / (TP + FP)

    negative_predictive_value = TN / (TN + FN)

    return sensitivity, specificity, positive_predictive_value, negative_predictive_value

def apply_multiple_logistic_regression(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


    logistic_model = LogisticRegression(max_iter=1000)  # Increase iterations as needed


    logistic_model.fit(X_train, y_train)


    y_pred = logistic_model.predict(X_test)


    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    # Evaluate the classifier
    accuracy = accuracy_score(y_test, y_pred)
    print(f'Accuracy: {accuracy:.2f}')
    sensitivity, specificity, ppv, npv = calculate_performance_metrics(y_test, y_pred)

    print(f"Sensitivity: {sensitivity:.2f}")
    print(f"Specificity: {specificity:.2f}")
    print(f"Positive Predictive Value: {ppv:.2f}")
    print(f"Negative Predictive Value: {npv:.2f}")

    # AUC
    y_probs = logistic_model.predict_proba(X_test)[:, 1]  # Get predicted probabilities for the positive class
    fpr, tpr, thresholds = roc_curve(y_test, y_probs)
    roc_auc = auc(fpr, tpr)
    # Return coefficients and feature namess
    coefs = logistic_model.coef_[0]
    feature_names = X.columns
    return coefs, feature_names

def analyze_risk_factors(coefs, feature_names):
    """Print the risk factors and their corresponding coefficients."""
    risk_factors = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coefs
    })


    risk_factors['Absolute Coefficient'] = risk_factors['Coefficient'].abs()
    risk_factors = risk_factors.sort_values(by='Absolute Coefficient', ascending=False)

    print("\nRisk Factors and their Coefficients:")
    print(risk_factors[['Feature', 'Coefficient', 'Absolute Coefficient']])
    risk_factors.to_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/RiskFactors.csv')


csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalHybridDATASET.csv'
X, y = load_data(csv_file_path)


coefs, feature_names = apply_multiple_logistic_regression(X, y)


analyze_risk_factors(coefs, feature_names)
'''

'''
def calculate_performance_metrics(Y_true, y_pred):

    cm = confusion_matrix(Y_true, y_pred)

    TN, FP, FN, TP = cm.ravel()  # True Negatives, False Positives, False Negatives, True Positives

    sensitivity = TP / (TP + FN)

    specificity = TN / (TN + FP)

    positive_predictive_value = TP / (TP + FP)


    negative_predictive_value = TN / (TN + FN)

    return sensitivity, specificity, positive_predictive_value, negative_predictive_value

KBest = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalHybridDATASET.csv')
X = KBest.drop(['stratified_risk'], axis=1)
Y = KBest['stratified_risk']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

Rfmodel = RandomForestClassifier(oob_score=False, n_estimators=95, min_samples_split=7, min_samples_leaf=2, max_features='sqrt', criterion='log_loss') # bootstrap model to avoid overfitting
Rfmodel.fit(X_train, Y_train) # once hyperparameters are determined by RCSV cell below, replace into Rfmodel

y_pred = Rfmodel.predict(X_test)
accuracy = accuracy_score(Y_test, y_pred)
mse = mean_squared_error(Y_test, y_pred)
mae = mean_absolute_error(Y_test, y_pred)
r2 = r2_score(Y_test, y_pred)
accuracy = accuracy_score(Y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(Y_test, y_pred))


sensitivity, specificity, ppv, npv = calculate_performance_metrics(Y_test, y_pred)

print(f"Sensitivity: {sensitivity:.2f}")
print(f"Specificity: {specificity:.2f}")
print(f"Positive Predictive Value: {ppv:.2f}")
print(f"Negative Predictive Value: {npv:.2f}")

    # AUC
y_probs = Rfmodel.predict_proba(X_test)[:, 1]  # Get predicted probabilities for the positive class
fpr, tpr, thresholds = roc_curve(Y_test, y_probs)
roc_auc = auc(fpr, tpr)

print(f'Accuracy: {accuracy}')
print(f'Mean Squared Error: {mse}')
print(f'Mean Absolute Error: {mae}')
'''

'''
KBest = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalHybridDATASET.csv')
X = KBest.drop(['stratified_risk'], axis=1)
Y = KBest['stratified_risk']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

rf_randomsearch = RandomForestClassifier()

param_tuning = {
    'n_estimators':np.arange(20,100,5), #start,stop,step intervals
    'criterion': ("gini", "entropy", "log_loss"),
    'min_samples_split': [2,3,5,7], # min samples to split internal node
    'min_samples_leaf': [1,2], # min samples at each leaf
    'max_features': ("sqrt","log2"),
    'oob_score': [True, False] # use bootstrap samples if true, oob_score prepared
}
# can alternatively apply gridsearch with ac gpu
rand_search = RSCV(estimator=rf_randomsearch, param_distributions=param_tuning, n_iter=100, cv=10, n_jobs=-1, verbose=2, scoring='balanced_accuracy')
rand_search.fit(X_train, Y_train)

best_rf = rand_search.best_estimator_
oobscore = best_rf.oob_score_ if best_rf.oob_score else None
y_pred = best_rf.predict(X_test)

mse = mean_squared_error(Y_test, y_pred)
mae = mean_absolute_error(Y_test, y_pred)
r2 = r2_score(Y_test, y_pred)

print(f"Best Hyperparameters: {rand_search.best_params_}")
print(f"Mean Squared Error: {mse}")
print(f"Mean Absolute Error: {mae}")
print(f"R2 Score: {r2}")
'''

'''
def load_data(csv_file_path):
    KBest = pd.read_csv(csv_file_path)

    if 'stratified_risk' not in KBest.columns:
        raise ValueError("'target' column not found in the DataFrame.")

    X = KBest.drop(['stratified_risk'], axis=1)
    Y = KBest['stratified_risk']

    return X, Y

def calculate_performance_metrics(Y_true, y_pred):

    cm = confusion_matrix(Y_true, y_pred)

    TN, FP, FN, TP = cm.ravel()  # True Negatives, False Positives, False Negatives, True Positives

    sensitivity = TP / (TP + FN)

    specificity = TN / (TN + FP)

    positive_predictive_value = TP / (TP + FP)

    negative_predictive_value = TN / (TN + FN)

    return sensitivity, specificity, positive_predictive_value, negative_predictive_value

def apply_knn(X, Y, n_neighbors=5):

    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    knn = KNeighborsClassifier(n_neighbors=n_neighbors)

    knn.fit(X_train, Y_train)

    y_pred = knn.predict(X_test)


    accuracy = accuracy_score(Y_test, y_pred)
    print(f'Accuracy: {accuracy:.2f}')
    print(classification_report(Y_test, y_pred))


    sensitivity, specificity, ppv, npv = calculate_performance_metrics(Y_test, y_pred)

    print(f"Sensitivity: {sensitivity:.2f}")
    print(f"Specificity: {specificity:.2f}")
    print(f"Positive Predictive Value: {ppv:.2f}")
    print(f"Negative Predictive Value: {npv:.2f}")

    # AUC
    y_probs = knn.predict_proba(X_test)[:, 1]  # Get predicted probabilities for the positive class
    fpr, tpr, thresholds = roc_curve(Y_test, y_probs)
    roc_auc = auc(fpr, tpr)

def main_knn_classification(csv_file_path, n_neighbors=5):
    """Main function to orchestrate KNN classification."""
    X, Y = load_data(csv_file_path)
    apply_knn(X, Y, n_neighbors)


csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalHybridDATASET.csv'
main_knn_classification(csv_file_path, n_neighbors=5)
'''

'''
def load_data(csv_file_path):
    KBest = pd.read_csv(csv_file_path)


    if 'stratified_risk' not in KBest.columns:
        raise ValueError("target column not found in the DataFrame.")


    X = KBest.drop(['stratified_risk'], axis=1)
    Y = KBest['stratified_risk']

    return X, Y

def calculate_performance_metrics(Y_true, y_pred):

    cm = confusion_matrix(Y_true, y_pred)

    TN, FP, FN, TP = cm.ravel()  # True Negatives, False Positives, False Negatives, True Positives

    sensitivity = TP / (TP + FN)

    specificity = TN / (TN + FP)

    positive_predictive_value = TP / (TP + FP)

    negative_predictive_value = TN / (TN + FN)

    return sensitivity, specificity, positive_predictive_value, negative_predictive_value

def apply_svm(X, Y):

    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)


    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    svm = SVC(kernel='linear', random_state=42, probability=True)  # Using a rbf kernel for multidimensionality

    svm.fit(X_train, Y_train)

    y_pred = svm.predict(X_test)


    accuracy = accuracy_score(Y_test, y_pred)
    print(f'Accuracy: {accuracy:.2f}')
    print(classification_report(Y_test, y_pred))


    sensitivity, specificity, ppv, npv = calculate_performance_metrics(Y_test, y_pred)

    print(f"Sensitivity: {sensitivity:.2f}")
    print(f"Specificity: {specificity:.2f}")
    print(f"Positive Predictive Value: {ppv:.2f}")
    print(f"Negative Predictive Value: {npv:.2f}")

    # AUC
    y_probs = svm.predict_proba(X_test)[:, 1]  # predicted probabilities for the positive class
    fpr, tpr, thresholds = roc_curve(Y_test, y_probs)
    roc_auc = auc(fpr, tpr)

def main_svm_classification(csv_file_path):

    X, Y = load_data(csv_file_path)
    apply_svm(X, Y)


csv_file_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalHybridDATASET.csv'

main_svm_classification(csv_file_path)
'''

'''
def load_data(csv_file_path):

    df = pd.read_csv(csv_file_path)
    return df

def z_score_normalization(df):
    columns_to_normalize = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'Patient-ID' in columns_to_normalize:
        columns_to_normalize.remove('Patient-ID')
    if 'sex' in columns_to_normalize:
        columns_to_normalize.remove('sex')
    if 'major_comorbidity' in columns_to_normalize:
        columns_to_normalize.remove('major_comorbidity')
    if 'node_positive_primary' in columns_to_normalize:
        columns_to_normalize.remove('node_positive_primary')
    if 'synchronous_crlm' in columns_to_normalize:
        columns_to_normalize.remove('synchronous_crlm')
    if 'multiple_metastases' in columns_to_normalize:
        columns_to_normalize.remove('multiple_metastases')
    if 'clinrisk_stratified' in columns_to_normalize:
        columns_to_normalize.remove('clinrisk_stratified')
    if 'bilobar_disease' in columns_to_normalize:
        columns_to_normalize.remove('bilobar_disease')
    if 'NASH_greater_4' in columns_to_normalize:
        columns_to_normalize.remove('NASH_greater_4')
    if 'NASH_score' in columns_to_normalize:
        columns_to_normalize.remove('NASH_score')
    if 'presence_sinusoidal_dilata' in columns_to_normalize:
        columns_to_normalize.remove('presence_sinusoidal_dilata')
    if 'steatosis_yesno' in columns_to_normalize:
        columns_to_normalize.remove('steatosis_yesno')
    if 'preoperative_pve' in columns_to_normalize:
        columns_to_normalize.remove('preoperative_pve')
    if 'chemo_before_liver_resection' in columns_to_normalize:
        columns_to_normalize.remove('chemo_before_liver_resection')
    if 'extrahep_disease' in columns_to_normalize:
        columns_to_normalize.remove('extrahep_disease')
    if 'Multiple_CLM' in columns_to_normalize:
        columns_to_normalize.remove('Multiple_CLM')
    if 'progression_or_recurrence' in columns_to_normalize:
        columns_to_normalize.remove('progression_or_recurrence')
    if 'fibrosis_greater_40_percent' in columns_to_normalize:
        columns_to_normalize.remove('fibrosis_greater_40_percent')
    if 'stratified_risk' in columns_to_normalize:
        columns_to_normalize.remove('stratified_risk')
    if 'max_diameter_5cm' in columns_to_normalize:
        columns_to_normalize.remove('max_diameter_5cm')

    scaler = StandardScaler()


    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

    return df

def main_z_score_normalization(input_csv_path, output_csv_path):

    df = load_data(input_csv_path)
    normalized_df = z_score_normalization(df)


    normalized_df.to_csv(output_csv_path, index=False)
    print(f"Normalized data saved to {output_csv_path}")


input_csv_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/LastClinicClean.csv'
output_csv_path = '/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/LastClinicClean.csv'

main_z_score_normalization(input_csv_path, output_csv_path)
'''

# Use cell to combine final radiomic data to final clinic data
# also used to combine numerous other datasets
'''
df1 = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalPCA.csv')
df2 = pd.read_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/LastClinicClean.csv')
merged_df = pd.merge(df1, df2, on='Patient-ID')
merged_df.to_csv('/content/drive/MyDrive/TCIA_CRLM_CT/manifest-1669817128730/FinalHybridDATASET.csv', index=False)
'''